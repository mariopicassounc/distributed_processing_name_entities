import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.FlatMapFunction;

import java.util.Arrays;
import java.util.List;

public class WordCountSpark {
    public static void main(String[] args) {
        // Crear el contexto de Spark
        JavaSparkContext sparkContext = new JavaSparkContext("local", "WordCountSpark");

        // Cargar los documentos de texto utilizando el lector de feeds
        List<String> documents = FeedReader.loadDocuments();

        // Crear un RDD a partir de la lista de documentos
        JavaRDD<String> documentsRDD = sparkContext.parallelize(documents);

        // Realizar el conteo de palabras en cada documento
        JavaRDD<String> wordsRDD = documentsRDD.flatMap((FlatMapFunction<String, String>) line ->
                Arrays.asList(line.split(" ")).iterator());

        // Realizar el conteo de palabras en todos los documentos
        JavaRDD<String> wordsCountRDD = wordsRDD.mapToPair(word -> new Tuple2<>(word, 1))
                .reduceByKey(Integer::sum);

        // Recopilar los resultados
        List<Tuple2<String, Integer>> wordCountList = wordsCountRDD.collect();

        // Mostrar los resultados
        for (Tuple2<String, Integer> wordCount : wordCountList) {
            System.out.println(wordCount._1() + ": " + wordCount._2());
        }

        // Computar entidades nombradas utilizando la funcionalidad existente
        List<String> namedEntities = NamedEntityRecognizer.computeNamedEntities(documents);

        // Mostrar las entidades nombradas encontradas
        for (String entity : namedEntities) {
            System.out.println("Named Entity: " + entity);
        }

        // Detener el contexto de Spark
        sparkContext.stop();
    }
}

